<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on tlehman@home</title>
    <link>/posts/index.xml</link>
    <description>Recent content in Posts on tlehman@home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Apr 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Seeing the bulk: a primer on dimensionality reduction</title>
      <link>/posts/seeing-the-bulk-a-primer-on-dimensionality-reduction/</link>
      <pubDate>Wed, 12 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/seeing-the-bulk-a-primer-on-dimensionality-reduction/</guid>
      <description>

&lt;p&gt;If you haven&amp;rsquo;t seen Interstellar, you might not get the reference in the title. Don&amp;rsquo;t sweat it, &lt;a href=&#34;http://interstellarfilm.wikia.com/wiki/Bulk_Beings&#34;&gt;the bulk beings&lt;/a&gt; are some advanced civilization that is able to move outside the familiar 3D space that we are stuck in. A similar plot device was used in Edwin Abbott&amp;rsquo;s book Flatland.&lt;/p&gt;

&lt;p&gt;Outside of entertainment, there is a lot of value in attempting to understand high dimensional spaces. Physics, machine learning, and engineering control systems all make use of higher-dimensional spaces in one way or another. I first got interested in higher dimensional spaces in high school, I saw a talk by a Russian physicist that introduced 4-dimensional cubes, and then made some passing comment that &amp;ldquo;there are six extra dimensions of space&amp;rdquo;, before closing up the lecture. That last part was confusing, but I later realized he was talking about &lt;a href=&#34;https://en.wikipedia.org/wiki/Calabi%E2%80%93Yau_manifold&#34;&gt;Calabi-Yau manifolds&lt;/a&gt; from string theory.&lt;/p&gt;

&lt;p&gt;In this post we will learn how datasets can be thought of as sets of points in a high dimensional space, and then we will learn how to map those sets into low (2 or 3)-dimensional spaces so that we can see them. The goal will be to preserve as much structure as possible, but still fit everything into a space we can see and feel.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start with 1D, real numbers are points on a line, easy. In 2D, we can use a pair (x,y) to name any point in a plane. For 3D, we can use triplets (x,y,z) to name points in a space like the one we physically live in.&lt;/p&gt;

&lt;p&gt;If we wanted to name the four corners of a tetrahedron, we could write out a table of data like this:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;x&lt;/th&gt;
&lt;th&gt;y&lt;/th&gt;
&lt;th&gt;z&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;But it&amp;rsquo;s not clear what that data looks like until we visualize it in 3D space:&lt;/p&gt;

&lt;script src=&#34;/js/three.min.js&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;tetrahedron&#34;&gt;&lt;/div&gt;
&lt;script&gt;
var scene = new THREE.Scene();
var camera = new THREE.PerspectiveCamera( 75, 1, 0.1, 1000 );

var renderer = new THREE.WebGLRenderer();
renderer.setSize( 300, 300 );
renderer.setClearColor( 0xfaf8f8, 1 );
document.getElementById(&#34;tetrahedron&#34;).appendChild( renderer.domElement );

var geometry = new THREE.Geometry();
var x1 = new THREE.Vector3(  1 ,  1 ,  1 )
var x2 = new THREE.Vector3( -1 , -1 ,  1 )
var x3 = new THREE.Vector3( -1 ,  1 , -1 )
var x4 = new THREE.Vector3(  1 , -1 , -1 )

geometry.vertices.push(x1);
geometry.vertices.push(x2);
geometry.vertices.push(x3);
geometry.vertices.push(x4);

var material = new THREE.PointsMaterial( { color: 0x888888, size: 0.25 } );
var points = new THREE.Points(geometry, material);

scene.add( points );

camera.position.z = 5;

function render() {
    requestAnimationFrame( render );
    points.rotation.x += 0.01;
    points.rotation.y += 0.01;
    renderer.render( scene, camera );
}
render();
&lt;/script&gt;

&lt;p&gt;We can extend the basic idea behind taking triples (x,y,z) and imagining them as points in a 3-dimensional space. Instead of triples, we can use n-tuples (x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, &amp;hellip;, x&lt;sub&gt;n&lt;/sub&gt;) and imagining them as points in an n-dimensional space. We can then measure distances between any two points (x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, &amp;hellip;, x&lt;sub&gt;n&lt;/sub&gt;) and (y&lt;sub&gt;1&lt;/sub&gt;, y&lt;sub&gt;2&lt;/sub&gt;, &amp;hellip;, y&lt;sub&gt;n&lt;/sub&gt;) by using the Pythagorean theorem. So all the tools we have for doing geometry with numbers (analytic geometry and linear algebra) work well, but we can&amp;rsquo;t directly visualize those spaces, since our brains evolved in a 3-dimensional environment.&lt;/p&gt;

&lt;p&gt;The good news is that we can &amp;ldquo;compress&amp;rdquo; higher dimensional shapes into 3D and still preserve plenty of information.&lt;/p&gt;

&lt;h2 id=&#34;principal-components-analysis&#34;&gt;Principal Components Analysis&lt;/h2&gt;

&lt;p&gt;One technique to reduce the dimensionality of a dataset is called Principal Components Analysis. It starts by modeling the dataset as a matrix, and then finds the directions in which the data varies the most. Those directions that contain the most variation coincide with the &amp;ldquo;principal components&amp;rdquo; of the matrix. For an excellent visual introduction to this technique, check out &lt;a href=&#34;http://setosa.io/ev/principal-component-analysis/&#34;&gt;Victor Powell&amp;rsquo;s post on PCA&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As an example, we will use a wine dataset&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:fn1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:fn1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; from 1991, it contains 13 properties (alcohol, malic acid, etc.) and 178 wines grown in the same region, but derived from three different plant varieties. Each property can be seen as a dimension, so using our spatial metaphor, we will imagine the 178 wines as points in a 13-dimensional space. Applying PCA will help us find the three most important directions, and then we can visualize those 178 points in a familiar 3D space. I&amp;rsquo;ll add colors to distinguish the wines from the three wine varieties. We should expect to see some structure, namely, the different plant varieties should form clusters.&lt;/p&gt;

&lt;div class=&#34;col-group&#34;&gt;

&lt;div&gt;

(X,Y)

&lt;img alt=&#34;wine pca X-Y&#34; src=&#34;/images/bulk/wine-pca-xy.png&#34;&gt;

&lt;/div&gt;

&lt;div&gt;
(X,Z)
&lt;img alt=&#34;wine pca X-Z&#34; src=&#34;/images/bulk/wine-pca-xz.png&#34;&gt;
&lt;/div&gt;

&lt;div&gt;
(Y,Z)
&lt;img alt=&#34;wine pca Y-Z&#34; src=&#34;/images/bulk/wine-pca-yz.png&#34;&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;From the (Y,Z) chart you can see that there are two main clusters, the blue and the (red/green), and from the (X,Z) chart you can see that the red clusters more towards the center of mass and the green is on the periphery. Also from the (X,Y) chart, the same pattern of &amp;ldquo;red inside&amp;rdquo;, &amp;ldquo;green outside&amp;rdquo; hold. This is a good example of how bringing your data into a lower dimensional space helps understanding and solving certain problems. Data that have these clusters can be fed into machine learning algorithms to do classification, or they can be used to find natural categories to organize data on its own terms.&lt;/p&gt;

&lt;script src=&#34;/js/three.min.js&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;winepca&#34;&gt;&lt;/div&gt;&lt;script&gt;
var wscene = new THREE.Scene();
var wcamera = new THREE.PerspectiveCamera( 75, 1, 0.1, 1000 );
var wrenderer = new THREE.WebGLRenderer();
wrenderer.setSize( 300, 300 );
wrenderer.setClearColor( 0xfaf8f8, 1 );
document.getElementById(&#39;winepca&#39;).appendChild( wrenderer.domElement );

var rgeo = new THREE.Geometry();
var ggeo = new THREE.Geometry();
var bgeo = new THREE.Geometry();
var w1 = new THREE.Vector3(  1 ,  1 ,  1 )
var w2 = new THREE.Vector3( -1 , -1 ,  1 )
var w3 = new THREE.Vector3( -1 ,  1 , -1 )
var w4 = new THREE.Vector3(  1 , -1 , -1 )

rgeo.vertices.push(w1);
ggeo.vertices.push(w2);
bgeo.vertices.push(w3);

var rmat = new THREE.PointsMaterial( { color: 0xFF0000, size: 0.25 } );
var gmat = new THREE.PointsMaterial( { color: 0x22BB22, size: 0.25 } );
var bmat = new THREE.PointsMaterial( { color: 0x0000FF, size: 0.25 } );

var rpts = new THREE.Points(rgeo, rmat);
var gpts = new THREE.Points(ggeo, gmat);
var bpts = new THREE.Points(bgeo, bmat);

wscene.add( rpts );
wscene.add( gpts );
wscene.add( bpts );

wcamera.position.z = 5;

function wrender() {
    requestAnimationFrame( wrender );
    rpts.rotation.x += 0.01;
    rpts.rotation.y += 0.01;
    bpts.rotation.x += 0.01;
    bpts.rotation.y += 0.01;
    gpts.rotation.x += 0.01;
    gpts.rotation.y += 0.01;
    wrenderer.render( wscene, wcamera );
}
wrender();
&lt;/script&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:fn1&#34;&gt;Wine Dataset &lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/Wine&#34;&gt;http://archive.ics.uci.edu/ml/datasets/Wine&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:fn1&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>word embeddings and semantics</title>
      <link>/blog/2016/09/04/word-embeddings-and-semantics/</link>
      <pubDate>Sun, 04 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016/09/04/word-embeddings-and-semantics/</guid>
      <description>

&lt;p&gt;Can a machine understand what a word means? Right now machines routinely correct spelling and grammar, but are pretty useless when it comes to semantics.&lt;/p&gt;

&lt;p&gt;Search engines are an exception, they have a rudimentary understanding of what words mean. One of the ways this can work is explored in Tomas Mikolov&amp;rsquo;s 2013 paper
on &lt;strong&gt;word embeddings&lt;/strong&gt;. Word embeddings are mappings from sets of words to vectors, such that the distances between the vectors represent the semantic similarity
of the words. These embeddings are learned by programs by scanning through large volumes of text, such as wikipedia articles and royalty-free books, and uses a
sliding context to adjust the parameters of the embedding.&lt;/p&gt;

&lt;p&gt;The goal is to learn a function &lt;code&gt;f&lt;/code&gt; from words to vectors such that the following equation holds:&lt;/p&gt;

&lt;p&gt;$$ f(\text{waiter}) - f(\text{man}) + f(\text{woman}) = f(\text{waitress}) $$&lt;/p&gt;

&lt;p&gt;Using the &amp;lsquo;skip-gram&amp;rsquo; technique in the Mikolov paper, we can get a pretty good function &lt;code&gt;f&lt;/code&gt;, for details, look at this &lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/tutorials/word2vec/index.html&#34;&gt;tensorflow tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I ran the &lt;code&gt;word2vec_optimized.py&lt;/code&gt; program and generated the word embeddings, and here is a real session from that model:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; model.analogy(&#39;man&#39;, &#39;woman&#39;, &#39;waiter&#39;)
&#39;waitress&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;word2vec_optimized.py&lt;/code&gt; script ran through all the text, generated the embeddings and then did that calculation up above to find the appropriate analogy. There is a lot more to
this that I want to explore, but so far it&amp;rsquo;s been surprising how well this works.&lt;/p&gt;

&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Efficient estimation of word representations in vector space by Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean (2013) &lt;em&gt;arXiv preprint arXiv:1301.3781&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Thing explainer in emacs</title>
      <link>/blog/2016/04/13/thing-explainer-in-emacs/</link>
      <pubDate>Wed, 13 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016/04/13/thing-explainer-in-emacs/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/tlehman/etc/blob/master/common-words.el&#34;&gt;This&lt;/a&gt; is my first &lt;a href=&#34;https://www.gnu.org/software/emacs/manual/html_node/emacs/Major-Modes.html&#34;&gt;major mode&lt;/a&gt; for emacs.
It was inspired by Randall Munroe&amp;rsquo;s &lt;a href=&#34;https://xkcd.com/thing-explainer/&#34;&gt;Thing Explainer&lt;/a&gt; and Morten Just&amp;rsquo;s &lt;a href=&#34;https://medium.com/@mortenjust/i-doomed-mankind-with-a-free-text-editor-ba6003319681#.utnh5bpjh&#34;&gt;editor that
doomed humanity&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The concept is simple, restrict your vocabulary to the 1000 most common words. If you can explain something using this
reduced vocabulary, then you really understand the topic. This is a decent test of understanding because it&amp;rsquo;s easy to
learn a word, and even use that word in the right context, but still have no idea how it relates to other things.&lt;/p&gt;

&lt;p&gt;This is not to say that specialized vocabulary is bad, it&amp;rsquo;s quite useful for communicating between people with a lot of
shared context. But when explaining things to people without that context, it&amp;rsquo;s important to be able to expand jargon words
into common words, even if it results in a larger, slightly less accurate version.&lt;/p&gt;

&lt;p&gt;Here is an animation of the editor mode in action:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/blogimg/common-word-mode.gif&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dockerize all the things</title>
      <link>/blog/2016/02/08/dockerize-all-the-things/</link>
      <pubDate>Mon, 08 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016/02/08/dockerize-all-the-things/</guid>
      <description>&lt;p&gt;Okay, maybe not &lt;em&gt;all&lt;/em&gt; the things, but the things that tend to litter your
filesystem with libraries, dependencies and other crap that increases the&lt;br /&gt;
chances of a conflict.&lt;/p&gt;

&lt;p&gt;If you are not familiar with &lt;a href=&#34;https://www.docker.com/what-docker&#34;&gt;Docker&lt;/a&gt;,
check it out, then come back here and troll my flaming fanboy drunk on kool aid
ravings in the comments.&lt;/p&gt;

&lt;p&gt;I write a lot of ruby and python code, and I also use a lot of ruby/python and
javascript code other people wrote as well. For ruby, I use the rbenv version
manager to isolate different versions of ruby from one another, this is because
some applications use features that are only present in newer versions, and also
I use bundler to isolate gems between different applications using the same ruby
version.&lt;/p&gt;

&lt;p&gt;This blog is generated from markdown using the octopress ruby project that
generates HTML from the markdown I write. My version uses ruby 1.9.3, which is
pretty old, but for a static site, there are no security risks, since by the
time the blog is published, it&amp;rsquo;s just a bunch of html files.&lt;/p&gt;

&lt;p&gt;On the python side, there are a lot of machine learning libraries and frameworks,
like GraphLab, caffee, TensorFlow, etc. that all link into statically compiled
C++ code. These take a long time to get set up manually, and they have a sprawling
web of dependencies spanning multiple languages. Virtualenv makes it possible to
isolate python projects from each other, but it&amp;rsquo;s yet another thing to remember
and takes more time just to use properly.&lt;/p&gt;

&lt;p&gt;Docker comes in as a more general way of isolating applications from one another.
Instead of having each language and application using some specialized isolation
tool, all applications can be run within docker containers, each of which is
completely unaware of any other container. Each of these containers shares the
same Linux kernel and hardware, so they are lightweight compared to virtual
machines. It is also possible to selectively allow containers to communicate,
that is what docker-compose is for.&lt;/p&gt;

&lt;p&gt;Since Docker uses Linux-kernel-specific features like
&lt;a href=&#34;https://en.wikipedia.org/wiki/Cgroups&#34;&gt;control groups&lt;/a&gt;, if you are running
Docker on any other operating system, you need to have a virtual machine, then
within that virtual machine, all the containers share that kernel. I recently
decided to get a high end laptop that runs linux, and it was specifically for
doing all my personal development work using Docker as much as made sense.&lt;/p&gt;

&lt;p&gt;So far I have my octopress blog, my goga.me (Go Game rails app), my Dato GraphLab
machine learning environment, a node.js environment (never installing npm on my host),
TensorFlow and the Google research deepdream code running in docker containers.
The last two can make use of
my GeForce GTX 970M with 1280 CUDA Cores, so it&amp;rsquo;s pretty sweet.&lt;/p&gt;

&lt;p&gt;There are things that I don&amp;rsquo;t think should be dockerized, like &lt;a href=&#34;http://fritzing.org/home/&#34;&gt;Fritzing&lt;/a&gt;
for electric circuits, and the Arduino IDE. Also my browser. So far it seems like
anything that can be installed and run out of the box without configurations or
numerous dependencies shouldn&amp;rsquo;t be made into containers.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d love to hear from more experienced Docker users about how it fits in their
workflows, I just got started, so I&amp;rsquo;m probably still in the excited noob stage to
really offer anything original.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New largest Mersenne prime found</title>
      <link>/blog/2016/01/20/new-largest-mersenne-prime-found/</link>
      <pubDate>Wed, 20 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016/01/20/new-largest-mersenne-prime-found/</guid>
      <description>&lt;p&gt;The largest prime number was just found, and it is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Mersenne_prime&#34;&gt;Mersenne prime&lt;/a&gt;, or a prime number of the form 2&lt;sup&gt;n&lt;/sup&gt; - 1. As of January 2016, 2&lt;sup&gt;74,207,281&lt;/sup&gt; - 1 is the largest known prime. Primes are interesting in their own right, but they also are indispensible in cryptography.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve known since Euclid that the number of primes is infinite, but it is still an open problem whether the number of Mersenne primes is infinite:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Conjecture   A Mersenne prime is a Mersenne number M&lt;sub&gt;n&lt;/sub&gt;  = 2&lt;sup&gt;p&lt;/sup&gt;  - 1  that is prime.
Are there infinite number of Mersenne Primes?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Source: &lt;a href=&#34;http://www.openproblemgarden.org/category/mersenne_prime&#34;&gt;Open Problem Garden&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Set up Mailinabox with LetsEncrypt</title>
      <link>/blog/2016/01/07/set-up-mailinabox-with-letsencrypt/</link>
      <pubDate>Thu, 07 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016/01/07/set-up-mailinabox-with-letsencrypt/</guid>
      <description>&lt;p&gt;If you are not familiar with the &lt;a href=&#34;https://eff.org&#34;&gt;EFF&lt;/a&gt; or their great project &lt;a href=&#34;https://letsencrypt.org&#34;&gt;LetsEncrypt&lt;/a&gt;,
don&amp;rsquo;t feel bad, even the &lt;a href=&#34;http://techcrunch.com/2016/01/07/who-the-f-is-the-eff-john-legere-wants-to-know/&#34;&gt;CEO of T-Mobile doesn&amp;rsquo;t know&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;But seriously, this post is about combining the best of two projects:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mailinabox.email/&#34;&gt;Mailinabox&lt;/a&gt; a standalone mail server that enables email as it should be&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://letsencrypt.org&#34;&gt;LetsEncrypt&lt;/a&gt; a non-profit TLS certificate authority&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Mailinabox allows anyone with little patience to set up their own mail server with all the fixings, web client,
spam filter, admin panel, etc. I recently just set it up, you can send email to &lt;img src=&#34;/images/email_address.png&#34; alt=&#34;mail at this domain&#34; /&gt; to contact me.&lt;/p&gt;

&lt;p&gt;Once you set up a domain name and a server and then get Mailinabox running, the next step is to get LetsEncrypt working.&lt;/p&gt;

&lt;p&gt;Since Mailinabox aims to automate almost everything, it is important to symlink the generated LetsEncrypt certificates to right place so Mailinabox knows where to find them.&lt;/p&gt;

&lt;p&gt;The directory &lt;code&gt;/home/user-data/ssl&lt;/code&gt; is created by Mailinabox, and to use LetsEncrypt certificates there, you will have to symlink them to that directory.&lt;/p&gt;

&lt;p&gt;I started by turning off nginx and running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ /letsencrypt-auto certonly --standalone -d tobilehman.com -d www.tobilehman.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That generated the following files in &lt;code&gt;/etc/letsencrypt/live/tobilehman.com/&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;cert.pem&lt;/li&gt;
&lt;li&gt;chain.pem&lt;/li&gt;
&lt;li&gt;fullchain.pem&lt;/li&gt;
&lt;li&gt;privkey.pem&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To use these in Mailinabox, you need to mkdir the directories under &lt;code&gt;/home/user-data/ssl/&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir /home/user-data/ssl/tobilehman.com
$ mkdir /home/user-data/ssl/www.tobilehman.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then symlink the &lt;code&gt;ssl_certificate.pem&lt;/code&gt; and &lt;code&gt;privkey.pem&lt;/code&gt; files to the right place:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ln -s /etc/letsencrypt/live/tobilehman.com/fullchain.pem /home/user-data/ssl/tobilehman.com/ssl_certificate.pem
$ ln -s /etc/letsencrypt/live/tobilehman.com/privkey.pem /home/user-data/ssl/tobilehman.com/ssl_private_key.pem

$ ln -s /etc/letsencrypt/live/tobilehman.com/fullchain.pem /home/user-data/ssl/www.tobilehman.com/ssl_certificate.pem
$ ln -s /etc/letsencrypt/live/tobilehman.com/privkey.pem /home/user-data/ssl/www.tobilehman.com/ssl_private_key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then finally, go to the directory that contains the mailinabox repository folder, and
run &lt;code&gt;mailinabox/tools/web_update&lt;/code&gt;, and then start nginx up again. You should be good to go! If not, email me with
some context, and we can figure out a solution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to set up LetsEncrypt (free SSL/TLS certs)</title>
      <link>/blog/2015/11/11/how-to-set-up-letsencrypt-free-ssl-slash-tls-certs/</link>
      <pubDate>Wed, 11 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015/11/11/how-to-set-up-letsencrypt-free-ssl-slash-tls-certs/</guid>
      <description>

&lt;p&gt;First, for those who don&amp;rsquo;t know what LetsEncrypt is, it is a project by the &lt;a href=&#34;https://www.eff.org/&#34;&gt;EFF&lt;/a&gt; to create a legitimate certificate authority that doesn&amp;rsquo;t charge. Up until now, certificate authorities charged, creating a financial barrier for many to use SSL/TLS to secure their site. It&amp;rsquo;s true that the prices weren&amp;rsquo;t unreasonable, but it&amp;rsquo;s just enough to prevent many people from choosing to reap the benefits of encryption. As far back as 2012, Jeff Atwood argued in a &lt;a href=&#34;http://blog.codinghorror.com/should-all-web-traffic-be-encrypted/&#34;&gt;blog post&lt;/a&gt; that we should make SSL default for web pages, but acknowledged that it would take a while for it to be the default:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Maybe not tomorrow, maybe not next year, but over the medium to long term, adopting encrypted web connections as a standard for logged-in users is the healthiest direction for the future of the web. We need to work toward making HTTPS easier, faster, and most of all, the default for logged in users.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now that LetsEncrypt has entered a limited beta stage, anyone who applied and got accepted can get free certificates, and only has to follow a few easy steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ssh into your webserver&lt;/li&gt;
&lt;li&gt;stop your web server program (nginx, apache, etc.)&lt;/li&gt;
&lt;li&gt;clone the letsencrypt git repo: &lt;code&gt;git clone https://github.com/letsencrypt/letsencrypt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;cd into &lt;code&gt;letsencrypt/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run the command: &lt;code&gt;./letsencrypt-auto --agree-dev-preview --server https://acme-v01.api.letsencrypt.org/directory auth&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;set up your certificate and private key with your web server&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;how-to-set-up-the-generated-certificates-with-your-web-server&#34;&gt;How to set up the generated certificates with your web server&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ve had to do this with Apache2 and nginx, and both times I couldn&amp;rsquo;t find good documentation, so here&amp;rsquo;s what worked for me:&lt;/p&gt;

&lt;h3 id=&#34;apache2&#34;&gt;Apache2&lt;/h3&gt;

&lt;p&gt;In the Apache config file for the website that is for your approved LetsEncrypt domain (tobilehman.com in my case), &lt;code&gt;/etc/apache2/sites-enabled/tobilehman.com.conf&lt;/code&gt;, just add:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;VirtualHost *:443&amp;gt;
     SSLEngine On
     SSLCertificateFile /etc/letsencrypt/live/tobilehman.com/cert.pem
     SSLCertificateKeyFile /etc/letsencrypt/live/tobilehman.com/privkey.pem
     SSLCertificateChainFile /etc/letsencrypt/live/tobilehman.com/chain.pem

     ServerAdmin examplemail@example.com
     ServerName tobilehman.com
     ServerAlias www.tobilehman.com
     DocumentRoot /var/www/tobilehman.com/public_html/
     ErrorLog /var/www/tobilehman.com/logs/error.log
     CustomLog /var/www/tobilehman.com/logs/access.log combined
&amp;lt;/VirtualHost&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where you replace &lt;code&gt;tobilehman.com&lt;/code&gt; with your domain name, and add your email for &lt;code&gt;ServerAdmin&lt;/code&gt; too.&lt;/p&gt;

&lt;h3 id=&#34;nginx&#34;&gt;nginx&lt;/h3&gt;

&lt;p&gt;In the nginx config file for the website that is for your approved LetsEncrypt domain (goga.me in my case), &lt;code&gt;/etc/nginx/nginx.conf&lt;/code&gt;, just add to following to the &lt;code&gt;http&lt;/code&gt; section:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server {
        listen 443;
        ssl on;

        ssl_certificate /etc/letsencrypt/live/goga.me/cert.pem;
        ssl_certificate_key /etc/letsencrypt/live/goga.me/privkey.pem;
        location / {
                proxy_pass http://127.0.0.1:8080;
                proxy_http_version 1.1;
                proxy_set_header Upgrade $http_upgrade;
                proxy_set_header Connection &amp;quot;upgrade&amp;quot;;
                proxy_read_timeout 3600;
                proxy_send_timeout 3600;
        }
        root /home/rails/goga.me;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where you replace &lt;code&gt;goga.me&lt;/code&gt; with your domain name.&lt;/p&gt;

&lt;p&gt;LetsEncrypt is just the beginning, but it&amp;rsquo;s a great practical first step towards a web that is SSL by default. Even for public content that doesn&amp;rsquo;t need the benefits of encryption, the integrity you get from SSL is worth it. If you are unfamiliar, this is the property of an SSL connection that you know that the page is from someone who controls the private key. To illustrate this, imagine that you visited &lt;code&gt;http://tobilehman.com&lt;/code&gt;, and that your router had some malware that redirected the HTTP request to an evil server that returned a fake tobilehman.com, if the connection was instead to &lt;code&gt;https://tobilehman.com&lt;/code&gt;, then any attempt to redirect that request to another evil server would be a dead giveaway, since that evil server can&amp;rsquo;t prove that it has access to the private key that is associated with &lt;code&gt;https://tobilehman.com&lt;/code&gt;. This alone is reason enough to use SSL everywhere, all the time. For everything. Even if securing the information from prying eyes isn&amp;rsquo;t a goal, the authenticity of the source is worth proving.&lt;/p&gt;

&lt;p&gt;The EFF is a great organization, and if you can, donate to them. And sign up for LetsEncrypt, encrypt all the things.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making a Go Game for the Web</title>
      <link>/blog/2015/11/03/making-a-go-game-for-the-web/</link>
      <pubDate>Tue, 03 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015/11/03/making-a-go-game-for-the-web/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve been feeling productive in the last few weeks. I just finished two Coursera classes, one in machine learning and one in Swift programming. I also finished my minimum viable go game: &lt;a href=&#34;http://goga.me&#34;&gt;goga.me&lt;/a&gt;. And we are working on a really exciting feature at work. I really like having lots of interesting things to work on. Anyway, enough feelings, you came here for games and code.&lt;/p&gt;

&lt;p&gt;The game of Go is ancient, it has very few rules, but an unfathomable level of complexity emerges from all the possible ways the pieces can be put on the board. The graphics were very straightforward to implement, it&amp;rsquo;s just a square grid with some colored circles, I don&amp;rsquo;t usually work with graphics, so I experimented with some different web technologies to see what worked best.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/blogimg/gogame_screenshot_0.png&#34; alt=&#34;example go game&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;attempt-1-html5-canvas&#34;&gt;Attempt 1, HTML5 Canvas&lt;/h2&gt;

&lt;p&gt;About a year ago I wrote a Go board using the HTML5 &amp;lt;canvas&amp;gt; element, &lt;a href=&#34;https://github.com/tlehman/go-websocket/blob/master/go_game.js&#34;&gt;here&amp;rsquo;s the source code&lt;/a&gt;, it doesn&amp;rsquo;t actually have a server side, I decided to ditch that approach, since canvas is really great for making arbitrarily complicated bitmap graphics, but I needed something much simpler. Also, when zooming in on a phone, the click event location was not the same as the location where the code drew a new piece.&lt;/p&gt;

&lt;h2 id=&#34;attempt-2-svg&#34;&gt;Attempt 2, SVG&lt;/h2&gt;

&lt;p&gt;After almost a year I came back to the project, I wrote out a rails backend for the data storage and game logic, and then learned SVG (Scalable Vector Graphics). Here&amp;rsquo;s &lt;a href=&#34;https://github.com/tlehman/goga.me/blob/b585f4dec45d726615d73fca301c261d2df17167/app/views/matches/show.html.erb&#34;&gt;the source code&lt;/a&gt;, what I like about SVG is that I can make the board positions part of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Document_Object_Model&#34;&gt;DOM&lt;/a&gt; and bind javascript events to the positions. This solves a problem with zooming, so that when I zoom in on a phone, the click event shows up in the right position.&lt;/p&gt;

&lt;h2 id=&#34;websockets-enabling-live-back-and-forth-gameplay&#34;&gt;WebSockets, enabling live back and forth gameplay&lt;/h2&gt;

&lt;p&gt;Another exciting technology that I spent some time learning about was &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API&#34;&gt;WebSockets&lt;/a&gt;, this allows two players to open a direct connection to one another, so that as soon as someone plays a piece, the other one knows. This way, the user does not have to refresh, it&amp;rsquo;s all live. I used the websockets-rails library to handle setting up a user-to-user connection through the rails backend. The &lt;a href=&#34;https://github.com/tlehman/goga.me/blob/master/app/views/matches/show.html.erb#L67-L80&#34;&gt;source code to handle binding and triggering is here&lt;/a&gt;. To see it in action, &lt;a href=&#34;http://goga.me/users/sign_up&#34;&gt;make an account at goga.me&lt;/a&gt;, and then &lt;a href=&#34;http://goga.me/matches/new&#34;&gt;create a match&lt;/a&gt;, and click &amp;lsquo;Start Match&amp;rsquo;, that will start a match against yourself, you can open it up on multiple devices, and when you start playing, the other screen will immediately update, it&amp;rsquo;s pretty cool!&lt;/p&gt;

&lt;h2 id=&#34;other-challenges&#34;&gt;Other challenges&lt;/h2&gt;

&lt;p&gt;After getting the graphics and networking to work correctly, the only other major challenge was handling capturing. Capturing is where one color completely surrounds the other color, and the surrounded pieces get removed, you can see it in the following screen capture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/blogimg/gogame_capturing.gif&#34; alt=&#34;example capturing in a go game&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The challenge is to determine the parts of the board that are connected. Since each piece can only be connected to it&amp;rsquo;s four neighboring pieces of the same color, you can start finding connected components by picking a point and walking to every piece connected to it that is the same color. A prime choice for this problem is a breadth first search:&lt;/p&gt;

&lt;h2 id=&#34;breadth-first-search&#34;&gt;Breadth First Search&lt;/h2&gt;

&lt;p&gt;To wrap my head around this algorithm I had to get out my trusty graph theory book. Here&amp;rsquo;s a summary of the algorithm and an example:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: An unweighted graph and a start vertex u&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Idea&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Maintain a set R of vertices that have been reached but not searched and&lt;/li&gt;
&lt;li&gt;a set S of vertices that have been searched.&lt;/li&gt;
&lt;li&gt;The set R is maintained as a First-In First-Out list (&lt;a href=&#34;https://en.wikipedia.org/wiki/Queue_%28abstract_data_type%29&#34;&gt;queue&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Initialization&lt;/strong&gt;: R = {u}, S = ø, d(u,u) = 0&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Iteration&lt;/strong&gt;: As long as R &amp;neq; ø, we search from the first vertex v of R. The neighbors of v not in (S U R) are added to the back of R and then v is removed from the front of R and placed in S.&lt;/p&gt;

&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;

&lt;p&gt;Let G be the adjacency graph of the following Go shape:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   1  2  3  4  5  6
 1    .  o  o  o  .
 2    .  .  .  .  .  &amp;lt;--- black component
 3    .
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;v&lt;/th&gt;
&lt;th&gt;R&lt;/th&gt;
&lt;th&gt;S&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;(2,1)&lt;/td&gt;
&lt;td&gt;[(2,1)]&lt;/td&gt;
&lt;td&gt;{}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;(2,1)&lt;/td&gt;
&lt;td&gt;[(2,2)]&lt;/td&gt;
&lt;td&gt;{(2,1)}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;(2,2)&lt;/td&gt;
&lt;td&gt;[(2,3),(3,2)]&lt;/td&gt;
&lt;td&gt;{(2,1),(2,2)}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;(2,3)&lt;/td&gt;
&lt;td&gt;[(3,2)]&lt;/td&gt;
&lt;td&gt;{(2,1),(2,2),(2,3)}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;(3,2)&lt;/td&gt;
&lt;td&gt;[(4,2)]&lt;/td&gt;
&lt;td&gt;{(2,1),(2,2),(2,3),(3,2)}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;(4,2)&lt;/td&gt;
&lt;td&gt;[(5,2)]&lt;/td&gt;
&lt;td&gt;{(2,1),(2,2),(2,3),(3,2),(4,2)}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;(5,2)&lt;/td&gt;
&lt;td&gt;[(6,2)]&lt;/td&gt;
&lt;td&gt;{(2,1),(2,2),(2,3),(3,2),(4,2),(5,2)}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;(6,2)&lt;/td&gt;
&lt;td&gt;[(6,1)]&lt;/td&gt;
&lt;td&gt;{(2,1),(2,2),(2,3),(3,2),(4,2),(5,2),(6,2)}&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;(6,1)&lt;/td&gt;
&lt;td&gt;[]&lt;/td&gt;
&lt;td&gt;{(2,1),(2,2),(2,3),(3,2),(4,2),(5,2),(6,2),(6,1)}&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br&gt; &lt;br&gt;&lt;/p&gt;

&lt;p&gt;Once I worked through the steps with this example and several others, writing the code to implement it was very straightforward, here&amp;rsquo;s the &lt;a href=&#34;https://github.com/tlehman/go-websocket/blob/8094890048ca845c0fef416573b8aba533ca6ea9/jasmine/src/ComponentMap.js#L16-L51&#34;&gt;javascript version&lt;/a&gt; and the &lt;a href=&#34;https://github.com/tlehman/goga.me/blob/b585f4dec45d726615d73fca301c261d2df17167/app/presenters/board_presenter.rb#L24-L41&#34;&gt;ruby version&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My go game is at a good stopping point for now, I still need to fix a few edge cases and add a chat feature. Also, I realized how satisfying it is to take a bunch of stuff I learned and make something real with it. I plan on making a native mobile client to this (iOS and Android), but that will take some more time and work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gun control and safety</title>
      <link>/blog/2015/10/01/gun-control-and-safety/</link>
      <pubDate>Thu, 01 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015/10/01/gun-control-and-safety/</guid>
      <description>&lt;p&gt;Another mass shooting happened today. I don&amp;rsquo;t usually write about this, but it happened in Roseberg, Oregon, close to where I live. Too close. I do understand that it is morally equivalent to a mass shooting in Ethiopia, Indonesia, the Netherlands or Australia, but something about events being close by have a bigger emotional impact.&lt;/p&gt;

&lt;p&gt;The question I want to address in this article is whether legislation can help solve this problem. I&amp;rsquo;ll focus on whether it is effective to implement barriers to gun ownership.&lt;/p&gt;

&lt;p&gt;The inspiration for this article is this graphic by Libby Isenstein:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://pbs.twimg.com/media/COF4FxPUkAEmOkI.jpg&#34; alt=&#34;Libby Isenstein&#39;s infographic on guns&#34; /&gt;&lt;/p&gt;

&lt;p&gt;My methodology is to see if there is a correlation between the number of gun-control-related laws in a state and the number of gun-related deaths. The columns in the above table will each count for up to one point. For example, if a state doesn&amp;rsquo;t require a permit for a gun, it gets 0, and if it does require a permit, it gets 1. For columns related to the difficulty of getting a gun, easy gets 0 points, moderate gets 0.5 points, and difficult gets 1 point.&lt;/p&gt;

&lt;p&gt;These points are then added up, and we look at all the states (plus DC), as pairs &lt;code&gt;(sum of points, gun deaths per 100,000 people)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/blogimg/guns_output_6_1.png&#34; alt=&#34;scatterplot&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This has a correlation coefficient of 0.52 (1.0 being a perfect fit), what that means is that there is a moderate trend between the degree of gun control and the number of gun-related deaths. It doesn&amp;rsquo;t prove that gun control laws cause fewer gun deaths, but it is at least consistent with that claim. And more importantly, it is evidence against the argument that more widespead access to guns make us all safer.&lt;/p&gt;

&lt;p&gt;No person should die for no reason. If legislation can help solve this problem (there&amp;rsquo;s reason to think it can), then it&amp;rsquo;s our duty to make that come to pass. Unfortunately, certain interpretations of the Second Amendment currently in force have prevented many sane limits on access to guns, this article by &lt;a href=&#34;https://www.washingtonpost.com/opinions/the-five-extra-words-that-can-fix-the-second-amendment/2014/04/11/f8a19578-b8fa-11e3-96ae-f2c36d2b1245_story.html?postshare=2541443818572894&#34;&gt;John Paul Stevens in the Washington Post explain a little history behind that&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For more detail on how to perform the analysis above, see below (requires familiarity with python, or a similar language).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import pandas as pd
import numpy as np
df = pd.read_csv(&amp;quot;guns.csv&amp;quot;)
df.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;deaths&lt;/th&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;laws&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;2.6&lt;/td&gt;
      &lt;td&gt;HI&lt;/td&gt;
      &lt;td&gt;7.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;3.1&lt;/td&gt;
      &lt;td&gt;MA&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;4.2&lt;/td&gt;
      &lt;td&gt;NY&lt;/td&gt;
      &lt;td&gt;6.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4.4&lt;/td&gt;
      &lt;td&gt;CT&lt;/td&gt;
      &lt;td&gt;4.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5.3&lt;/td&gt;
      &lt;td&gt;RI&lt;/td&gt;
      &lt;td&gt;4.5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
%matplotlib inline
plt.ylabel(&amp;quot;gun deaths per 100k&amp;quot;)
plt.xlabel(&amp;quot;number of gun laws&amp;quot;)
plt.scatter(df[&amp;quot;laws&amp;quot;], df[&amp;quot;deaths&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection at 0x10d678850&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/images/blogimg/guns_output_3_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since it looks like a downward trend, let&amp;rsquo;s see how well a linear model fits this data.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
from sklearn import linear_model
reg = linear_model.LinearRegression() 
X = df[&#34;laws&#34;].values.reshape((len(df),1))
y_true = df[&#34;deaths&#34;].values.reshape((len(df),1))

reg.fit(X,y_true)
y_pred = reg.predict(X)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.ylabel(&amp;quot;gun deaths per 100k&amp;quot;)
plt.xlabel(&amp;quot;number of gun laws&amp;quot;)
plt.plot(X,y_pred)
plt.scatter(X,y_true)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.collections.PathCollection at 0x10e980fd0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/images/blogimg/guns_output_6_1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It looks like a really good, fit, but to make it precist, we should find the $R^2$ score.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import r2_score
r2_score(y_true, y_pred)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.52124939556633598
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Review: The Warmth of Other Suns by Isabel Wilkerson</title>
      <link>/blog/2015/08/04/review-the-warmth-of-other-suns-by-isabel-wilkerson/</link>
      <pubDate>Tue, 04 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015/08/04/review-the-warmth-of-other-suns-by-isabel-wilkerson/</guid>
      <description>

&lt;p&gt;Isabel Wilkerson&amp;rsquo;s riveting book about the Great Migration of African-Americans out of the south is the perfect blend of narrative and statisitcs.&lt;/p&gt;

&lt;div style=&#34;float: right; width: 50%&#34;&gt;
  &lt;img src=&#34;/images/books/warmth_of_other_suns.png&#34;&gt;
&lt;/div&gt;

&lt;p&gt;The personal stories of Ida Mae Brandon Gladney, George Swanson Starling and Robert Joseph Pershing Foster are the substantive bulk of the book. Around these personal stories are statistics that serve to add context and clarity to a movement that was so large, and took place over a long enough time, that it&amp;rsquo;s easy to miss it as a distinct historical period.&lt;/p&gt;

&lt;p&gt;This book was my introduction to the Great Migration. It was a migration that began in 1915 with World War 1 and ended around 1970. Nearly 6 million African Americans left the South and traveled to cities in the North or the West, such as New York, Chicago, Baltimore, Cleveland, Los Angeles and Oakland. It wasn&amp;rsquo;t a coordinated effort, it was the result of large economic and social forces, as well as millions of personal decisions to leave a part of the United States that historically had treated black people as property, and hadn&amp;rsquo;t improved much since the end of the civil war.&lt;/p&gt;

&lt;p&gt;Several decades after the end of the civil war, even though slavery had been abolished, the old social order still existed, the same industries existed. Slavery had been replaced with the slightly less bad, but still quite terrible practice of sharecropping, where white planation owners would hire former slaves and children of former slaves as laborers. They usually took advantage of the fact that the black workers weren&amp;rsquo;t educated, effectively getting free or near-free labor. The de facto segregation that existed quickly became de jure segregation with the passing of Jim Crow laws in most southern states and counties. This began to change in a big way during World War 1, since the men that left for war in the north left a labor vacuum. Recruiters from the north would go south, trying to convince many black residents in the south to move north for jobs. This was just the beginning, since the family and friends of those first migrants kept that route in their minds as a possible option for a better life.&lt;/p&gt;

&lt;p&gt;The book proceeds by moving through three parallel timelines, one of Ida Mae, a sharecropper&amp;rsquo;s wife from Missippi, who left in the 1930s. Then George Starling, a fruit picker from Florida, and finally Robert Foster, from Lousiana. The stories are told in parallel, but they all take place about a decade apart from one another. Wilkerson&amp;rsquo;s narrative style is so good, I sometimes felt like the stories were happening in the present.&lt;/p&gt;

&lt;p&gt;In between the personal stories were discussions relevant statistics, such as how many people had left which state, and incomes of black teachers and white teachers that were similarly qualified. The book does doesn&amp;rsquo;t just steep the reader in data, it moves briskly back into narrative and continues the unfolding saga of the three brave migrants.&lt;/p&gt;

&lt;p&gt;There were two parts of the book that stood out most to me, one having to do with wealth inequality:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In neighboring Mississippi, white teachers and principals were making $630 a year, while the colored ones were paid a third of that-$215 a year, hardly more than field hands. But knowing that didn&amp;rsquo;t ease the burden of the Foster&amp;rsquo;s lives, get their children through college, or allow them to build assets to match their status and education.
The disparity in pay, reported without apology in the local papers for all to see, would have far-reaching effects. It would mean that the most promising of colored people, having received next to nothing in material assets from their slave foreparents, had to labor with the knowledge that they were now being underpaid by more than half, that they were so behind it would be all but impossible to accumulate the assets their white counterparts could, and that they would, by definition, have less to leave succeeding generations than similar white families.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This was followed by the long term consequences, which influence the world of today (2015):&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Multiplied over the generations, it would mean a wealth deficit between the races that would require a miracle windfall or near aceticism on the part of colored families if they were to have any chance of catching up or amassing anything of value. Otherwise, the chasm would continue, as it did for blacks as a group even into the succeeding century. The layers of accumulated assets built up by the better-paid dominant caste, generation after generation, would factor into a wealth disparity of white Americans having an average net worth ten times that of black Americans by the turn of the twenty-first century, dampening the economic prospects of the children and grandchildren of the Great Migration before they were even born.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This explanation clarified the concept of white privilege to me better than anything I&amp;rsquo;ve seen before, or since. In fact, it&amp;rsquo;s gotten slightly worse since the turn of the century, a recent Pew Research study found that the average net worth of white families is thirteen times that of the average black family:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.pewresearch.org/files/2014/12/FT_14.12.11_wealthGap2.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;The other part that stood out to me was about automation and jobs, the story about &lt;a href=&#34;https://en.wikipedia.org/wiki/Technological_unemployment&#34;&gt;technological unemployment&lt;/a&gt; usually goes: &lt;em&gt;first a new technology emerges that makes some kind of human labor obsolete, and then those laborers lose their jobs, and get replaced by machines&lt;/em&gt;. While there are many examples of this, reality is much more complicated. Here&amp;rsquo;s the quote from the book related to this topic:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It took World War II and the even bigger outflow of blacks to awaken them to what some agricultural engineers working on a mechanical harvester already knew: &amp;ldquo;Much of this labor is not returning to the farm,&amp;rdquo; Harris P. Smith, the chief of agricultural engineering at Texas A&amp;amp;M University, wrote in 1946. &amp;ldquo;Therefore, the cotton farmer is forced to mechanize.&amp;rdquo; As for the connection between the Migration and the machine, Smith concluded that &amp;ldquo;instead of the machines displacing labor, they were used to replace the labor that had left the farm&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the case of cotton picking in the south, automation didn&amp;rsquo;t destroy jobs, it filled in a void of jobs that people willingly left. This aligns with my ideas about what kinds of work should be replaced by machines. Anything repetitive and degrading should be done by a machine. Humans are too important.&lt;/p&gt;

&lt;p&gt;The book is well worth your time, it is very informative, eye-opening, and incredibly well written.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;

&lt;p&gt;[1] Wilkerson, Isabel. The Warmth of Other Suns: The Epic Story of America&amp;rsquo;s Great Migration. New York, NY: Random House, 2010. Print.&lt;/p&gt;

&lt;p&gt;[2] Wealth inequality has widened along racial, ethnic lines since end of Great Recession &lt;a href=&#34;http://www.pewresearch.org/fact-tank/2014/12/12/racial-wealth-gaps-great-recession/&#34;&gt;http://www.pewresearch.org/fact-tank/2014/12/12/racial-wealth-gaps-great-recession/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Please Steal This Blog Post</title>
      <link>/blog/2015/07/15/please-steal-this-blog-post/</link>
      <pubDate>Wed, 15 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015/07/15/please-steal-this-blog-post/</guid>
      <description>&lt;p&gt;I believe that free access to information is a &lt;a href=&#34;http://www.un.org/en/documents/udhr/index.shtml#a19&#34;&gt;human right&lt;/a&gt;. The Internet has dramatically enhanced our ability to exercise this right, but unfortunately most humans cannot access the Internet. Today, over 4.3 billion people cannot connect to the Internet at all and another roughly 1 billion people have their Internet connections censored or monitored. A world where only 20% of humans have truly free access to digital information is unacceptable.&lt;/p&gt;

&lt;p&gt;That is why I support the creation of Humanity&amp;rsquo;s Public Library, an initiative by Outernet. Outernet broadcasts a data signal from satellites that is free to receive anywhere on Earth. While this is not an Internet connection, it is a free stream of critical information. What information is considered &amp;ldquo;critical?&amp;rdquo; &lt;a href=&#34;https://wiki.outernet.is/wiki/Edit-a-thon&#34;&gt;You decide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Outernet and &lt;a href=&#34;http://creativecommons.org/weblog/entry/45749&#34;&gt;Creative Commons&lt;/a&gt; are co-hosting the first edit-a-thon for Humanity&amp;rsquo;s Public Library on July 18-19 2015 to decide what is included in this library. Anyone on Earth is encouraged to participate - details on how to have your voice heard in this process can be found at &lt;a href=&#34;http://editathon.outernet.is&#34;&gt;http://editathon.outernet.is&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I want to encourage our users to submit their own work and to submit content that is licensed for redistribution. One such work is this very blog post. Copy these words and post them on your own blog and let&amp;rsquo;s all gather together and build a #LibraryFromSpace.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This blog post is licensed under &lt;a href=&#34;https://creativecommons.org/publicdomain/zero/1.0/&#34;&gt;CC0&lt;/a&gt; and is free to be distributed and edited without restriction&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Drunk Driving</title>
      <link>/blog/2015/04/23/bayesian-drunk-driving/</link>
      <pubDate>Thu, 23 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015/04/23/bayesian-drunk-driving/</guid>
      <description>

&lt;p&gt;Driving drunk is illegal for a good reason, it&amp;rsquo;s way riskier than driving sober. This article isn&amp;rsquo;t about driving drunk
though, it&amp;rsquo;s more about the sloppy thought processes that can too easily confuse something as obvious as that first
sentence. Here&amp;rsquo;s an example of a bogus argument that appears to support the idea that drunk driving is actually safer:&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;
&lt;p&gt;From a recent talk: &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; of accidents involve drunk drivers, so &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; don’t =&amp;gt; sober drivers 2× as bad.&lt;/p&gt;
&amp;mdash; Colin Beveridge (@icecolbeveridge)
&lt;a href=&#34;https://twitter.com/icecolbeveridge/status/587317304335147008&#34;&gt;April 12, 2015&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So the argument is as follows: In 2012, 10,322 people were killed in alcohol-impaired driving crashes,
accounting for nearly one-third (31%) of all traffic-related deaths in the United States [1].
That means that approximately one third of traffic-related deaths involve drunk driving, meaning that
two thirds of traffic-related deaths don&amp;rsquo;t involve drunk driving. Therefore, sober drivers are twice as
likely to die in a traffic accident.&lt;/p&gt;

&lt;p&gt;If you think something is wrong with that argument, you are right, but it&amp;rsquo;s not just because the conclusion
intuitively seems wrong, it&amp;rsquo;s because it involves a mistake in conditional probability. To see the mistake,
it helps to introduce a litle notation, we will define:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;P(D) to be the probability that a person is drunk&lt;/li&gt;
&lt;li&gt;P(A) to be the probability that a person will die in a traffic-related accident&lt;/li&gt;
&lt;li&gt;P(D | A) &lt;em&gt;(pronounced probability of D given A)&lt;/em&gt; is the probability that a person is drunk, given that
there was a death in a traffic-related accident they were in&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So using the 2012 CDC data, we can assign 31%, P(D | A) = 0.31. This is that the probability of a drunk
driver being involved &lt;strong&gt;given that there was a deadly driving accident&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The first thing to point out is that the statement that &amp;lsquo;sober drivers are twice as likely as drunk drivers
to die in an accident&amp;rsquo; is really a statement about P(A | D), that is, the probability of a deadly driving
accident &lt;strong&gt;given that that person is drunk&lt;/strong&gt;. We don&amp;rsquo;t know this yet, however, we can figure it out using
Bayes&amp;rsquo; theorem.&lt;/p&gt;

&lt;h2 id=&#34;bayes-theorem&#34;&gt;Bayes&amp;rsquo; Theorem&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34;&gt;Bayes&amp;rsquo; Theorem&lt;/a&gt; is unusual in that it is extremely useful
and easy to prove, but hard to really understand.
This is something I learned several times in college, but never really understood it&amp;rsquo;s importance until much
later. To see how easy to prove it is, we go back to the definition of conditional probability:&lt;/p&gt;

&lt;p&gt;{% latex %}
$P(X|Y) = P(X \cap Y)/P(Y)$
{% endlatex %}&lt;/p&gt;

&lt;p&gt;Where P(X &amp;cap; Y) is the probability of X and Y occurring. Since this is true for any pair of events X and Y,
we can reverse them and get&lt;/p&gt;

&lt;p&gt;{% latex %}
$P(Y|X) = P(Y \cap X)/P(X)$
{% endlatex %}&lt;/p&gt;

&lt;p&gt;Also, remember that AND is commutative, so that P(X &amp;cap; Y) = P(Y &amp;cap; X), so we can multiply the above two
equations by P(Y) and P(X), respectively, to get:&lt;/p&gt;

&lt;p&gt;{% latex %}
$P(X|Y)P(Y) = P(X \cap Y) = P(Y \cap X) = P(Y|X)P(X)$
{% endlatex %}&lt;/p&gt;

&lt;p&gt;This relates P(X|Y) to P(Y|X), P(X) and P(Y), we can solve the above equation to get:&lt;/p&gt;

&lt;p&gt;{% latex %}
$P(X|Y) = {P(Y|X)P(X)\over P(Y)}$
{% endlatex %}&lt;/p&gt;

&lt;p&gt;And that&amp;rsquo;s it, we took the definition of conditional probability, did a little algebra, and out popped Bayes&amp;rsquo;
theorem, we can now apply this to the above drunk driving fallacy, and calculate the probability that we are
interested in, that is, P(A | D).&lt;/p&gt;

&lt;p&gt;{% latex %}
$P(A|D) = {P(D|A)P(A)\over P(D)}$
{% endlatex %}&lt;/p&gt;

&lt;p&gt;Since we know P(D|A), we just need to find P(A) and P(D). Since the CDC data we are using is annual data,
we need to take the number of casualties from deadly accidents in the United States for the year of 2012 (33,561)
and divide by the number of drivers (211,814,830), that gives an estimate of P(A) = 33,&lt;sup&gt;561&lt;/sup&gt;&amp;frasl;&lt;sub&gt;211&lt;/sub&gt;,814,830 =
0.0001584, which is about 1 in 6,313.&lt;/p&gt;

&lt;p&gt;Next, we need to find the probability that a driver is drunk P(D), we will use the data from the study
referenced in [3], and define &amp;lsquo;drunk&amp;rsquo; to be a BAC of &amp;geq; 0.1%. Then P(D) = 0.00387 or about 1 in 258 (more
on this calculation in the notes below).&lt;/p&gt;

&lt;p&gt;Now that we have:&lt;/p&gt;

&lt;p&gt;P(D|A) = 0.31 (* probability of a driver being drunk, given they were involved in an accident where someone died *),&lt;/p&gt;

&lt;p&gt;P(A) = 0.0001584 (* probability of a driver being involved in an accident where someone died *), and&lt;/p&gt;

&lt;p&gt;P(D) = 0.00387 (* probability of a driver being drunk *)&lt;/p&gt;

&lt;p&gt;We can figure out P(A|D) (* probability of a drunk driver getting into a deadly accident *)&lt;/p&gt;

&lt;p&gt;P(A|D) = P(D|A)P(A)/P(D) = (0.31*0.0001584)/0.00387 = 0.0127 (1.27 %)&lt;/p&gt;

&lt;p&gt;1.27% is not insignificant, it&amp;rsquo;s about half the probability of rolling snake eyes in craps.
Now, let&amp;rsquo;s compare that to sober driving, we just need to calculate P(A|D&lt;sup&gt;c&lt;/sup&gt;). We can use &lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_total_probability&#34;&gt;Kolmogorov&amp;rsquo;s
Theorem of total probability&lt;/a&gt;, shuffle a few terms to
get:&lt;/p&gt;

&lt;p&gt;P(A|D&lt;sup&gt;c&lt;/sup&gt;) = (P(A) - P(A|D)P(D))/P(D&lt;sup&gt;c&lt;/sup&gt;) = (0.0001584 - 0.0127*0.00387)/(1-0.00387) = .000109,
which is about 1 in 9118.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So the probability of getting in a deadly accident, given that you are drunk is 1.27%, and the probability of getting into
a deadly accident, given that you are not drunk is .01%, that means that it is 127 times more likely that you will get into
a deadly accident while drunk.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;[1] Impaired Driving: Get the Facts &lt;em&gt;Centers for Disease Control&lt;/em&gt;
&lt;a href=&#34;http://www.cdc.gov/Motorvehiclesafety/impaired_driving/impaired-drv_factsheet.html&#34;&gt;
[link]
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] Total licensed drivers &lt;em&gt;U.S. Department of Transportation Federal Highway Administration&lt;/em&gt;
&lt;a href=&#34;http://www.fhwa.dot.gov/policyinformation/statistics/2012/dl22.cfma&#34;&gt;
[link]
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Probability of arrest while driving under the influence (George A Beitel, Michael C Sharp, William D Glauz)
&lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1730617/pdf/v006p00158.pdf&#34;&gt;
[link]
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes on [3], we don&amp;rsquo;t technically have P(D), but we do have P(D|A&lt;sub&gt;1&lt;/sub&gt;), P(A&lt;sub&gt;1&lt;/sub&gt;),
and P(A&lt;sub&gt;1&lt;/sub&gt;|D), where A&lt;sub&gt;1&lt;/sub&gt; is the event that a person is arrested. We can then find
P(D) = (P(D|A&lt;sub&gt;1&lt;/sub&gt;)P(A&lt;sub&gt;1&lt;/sub&gt;))/P(A&lt;sub&gt;1&lt;/sub&gt;|D) = (0.06&amp;times;0.000374)/0.0058 =
.00387&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Word frequencies after removing common words</title>
      <link>/blog/2015/02/10/word-frequencies-after-removing-common-words/</link>
      <pubDate>Tue, 10 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015/02/10/word-frequencies-after-removing-common-words/</guid>
      <description>

&lt;p&gt;In taking the &lt;a href=&#34;https://class.coursera.org/mmds-002&#34;&gt;Coursera class on Mining Massive Datasets&lt;/a&gt;, the problem of computing word frequency for very large documents came up. I wanted some convenient tools for breaking documents into streams of words, and also a tool to remove common words like &amp;lsquo;the&amp;rsquo;, so I wrote up &lt;code&gt;words&lt;/code&gt; and &lt;code&gt;decommonize&lt;/code&gt;. The &lt;code&gt;decommonize&lt;/code&gt; script is just a big &lt;code&gt;grep -v &#39;(foo|bar|baz)&#39;&lt;/code&gt;, where the words foo, bar and baz come from the words in a file. I made a script &lt;code&gt;generate_decommonize&lt;/code&gt; that reads in a list of common words, and builds the regex for &lt;code&gt;grep -v&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;example-usage-of-words-and-decommonize&#34;&gt;Example usage of &lt;code&gt;words&lt;/code&gt; and &lt;code&gt;decommonize&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;The full source code is available &lt;a href=&#34;https://github.com/tlehman/words&#34;&gt;here on github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After running &lt;code&gt;make install&lt;/code&gt;, you should have &lt;code&gt;words&lt;/code&gt; and &lt;code&gt;decommonize&lt;/code&gt; in your PATH, you can use them to find key words that are characteristic of a document, I chose&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the U.S. Declaration of Independence:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;
$ words &lt; declaration_of_independence.txt | decommonize  | sort | uniq -c | sort -n | tail
   4 time
   5 among
   5 most
   5 powers
   6 government
   6 such
   7 right
   8 states
   9 laws
  10 people
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Sherlock Holmes&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;
$ words &lt; doyle_sherlock_holmes.txt | decommonize  | sort | uniq -c | sort -n | tail
 174 think
 175 more
 177 over
 212 may
 212 should
 269 little
 274 mr
 288 man
 463 holmes
 466 upon
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Working with Unix Processes (by @jstorimer)&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;
$ words &lt; working_with_unix_processes.txt | decommonize  | sort | uniq -c | sort -n | tail
  73 signal
  82 system
  88 ruby
  90 exit
 100 code
 100 parent
 143 its
 146 child
 184 processes
 444 process
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So &lt;code&gt;words&lt;/code&gt; breaks up the document into lower-case alphabetic words, then &lt;code&gt;decommonize&lt;/code&gt; greps out the common words, and &lt;code&gt;sort&lt;/code&gt; and &lt;code&gt;uniq -c&lt;/code&gt; are used to count instances of each decommonized word, and then the results are sorted.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>White House releases first ever open source budget proposal</title>
      <link>/blog/2015/02/03/white-house-releases-first-ever-open-source-budget-proposal/</link>
      <pubDate>Tue, 03 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015/02/03/white-house-releases-first-ever-open-source-budget-proposal/</guid>
      <description>&lt;p&gt;The White House just released the &lt;a href=&#34;https://github.com/WhiteHouse/2016-budget-data&#34;&gt;first ever open source budget proposal&lt;/a&gt;. It is released on GitHub, and it&amp;rsquo;s a bunch of CSV files. This is not very difficult, it requires only a few extra clicks when exporting an Excel spreadsheet, but hosting it on GitHub also opens it up to &lt;a href=&#34;https://help.github.com/articles/using-pull-requests/&#34;&gt;Pull Requests&lt;/a&gt;, which I&amp;rsquo;ve &lt;a href=&#34;/blog/2013/09/14/viewing-nsa-accountability-act-amendments-as-a-diff/&#34;&gt;talked about before&lt;/a&gt; as being a much better tool for 21st century democracy. Instead of paper and a bunch of politicians in a room following procedure, we should intead have a digital system where all citizens can contribute as easily as they can update a facebook status or apply an instagram filter.&lt;/p&gt;

&lt;p&gt;One huge caveat is in order though: there is no reason to assume that the White House and Congress will even consider pull requests, let alone apply them. This aside, I will experiment with this, I&amp;rsquo;ve already modified &lt;a href=&#34;https://github.com/dinedal/textql/pull/39&#34;&gt;textql&lt;/a&gt; so that I can easily query these CSV files from a SQLite database. If I have an idea about how I&amp;rsquo;d like to change the budget, I&amp;rsquo;ll submit the pull request and then follow it&amp;rsquo;s response, if any.&lt;/p&gt;

&lt;p&gt;Caveats aside, I am impressed with the choice of technologies for making these public issues more accessible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parsing nested expressions using Bison</title>
      <link>/blog/2015/01/27/parsing-nested-expressions-using-bison/</link>
      <pubDate>Tue, 27 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015/01/27/parsing-nested-expressions-using-bison/</guid>
      <description>&lt;p&gt;I modified my &lt;a href=&#34;/blog/2015/01/22/tip-calculation-using-bison-grammar/&#34;&gt;tipcalc&lt;/a&gt; program to handle expressions of arbitrary depth, so now it can handle input like &lt;code&gt;((($100 + 2%) + 2%) - 3%) + 3.5%&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The trick was to change the &lt;code&gt;start&lt;/code&gt; symbol to match &lt;code&gt;binary_expression&lt;/code&gt;, and then define &lt;code&gt;binary_expression&lt;/code&gt; recursively, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;binary_expression:
    dollars OP_PLUS percentage
    |
    dollars OP_MINUS percentage
    |
    LPAREN binary_expression RPAREN OP_PLUS percentage
    |
    LPAREN binary_expression RPAREN OP_MINUS percentage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is what makes this new version a context-free grammar and not a regular grammar. Now, if you think that you could still handle this input with a regular expression, notice that adding percentages is not associative. For example, you might think we could drop the parens and just parse &lt;code&gt;$100 + 2% + 2% + 2%&lt;/code&gt; using &lt;code&gt;/\$\d+ (\+ \d\%)+/&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    \$\d+ (\+ \d\%)+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://www.debuggex.com/i/EaZiAO8PWJosT0b_.png&#34; alt=&#34;Regular expression visualization&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.debuggex.com/r/EaZiAO8PWJosT0b_&#34;&gt;Debuggex Demo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, if instead we wrote &lt;code&gt;$100 + 2% - 2% + 2%&lt;/code&gt;, associativity says we can reduce it to &lt;code&gt;$100 + 2%&lt;/code&gt;, however, when associated to the left &lt;code&gt;(($100 + 2%) - 2%) + 2%&lt;/code&gt; it is clear that the result is different from &lt;code&gt;$100 + 2%&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>