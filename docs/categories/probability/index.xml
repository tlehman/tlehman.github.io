<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Probability on tlehman@home</title>
    <link>/categories/probability/index.xml</link>
    <description>Recent content in Probability on tlehman@home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/categories/probability/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Bayesian Drunk Driving</title>
      <link>/blog/2015/04/23/bayesian-drunk-driving/</link>
      <pubDate>Thu, 23 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015/04/23/bayesian-drunk-driving/</guid>
      <description>

&lt;p&gt;Driving drunk is illegal for a good reason, it&amp;rsquo;s way riskier than driving sober. This article isn&amp;rsquo;t about driving drunk
though, it&amp;rsquo;s more about the sloppy thought processes that can too easily confuse something as obvious as that first
sentence. Here&amp;rsquo;s an example of a bogus argument that appears to support the idea that drunk driving is actually safer:&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;
&lt;p&gt;From a recent talk: &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; of accidents involve drunk drivers, so &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; don’t =&amp;gt; sober drivers 2× as bad.&lt;/p&gt;
&amp;mdash; Colin Beveridge (@icecolbeveridge)
&lt;a href=&#34;https://twitter.com/icecolbeveridge/status/587317304335147008&#34;&gt;April 12, 2015&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So the argument is as follows: In 2012, 10,322 people were killed in alcohol-impaired driving crashes,
accounting for nearly one-third (31%) of all traffic-related deaths in the United States [1].
That means that approximately one third of traffic-related deaths involve drunk driving, meaning that
two thirds of traffic-related deaths don&amp;rsquo;t involve drunk driving. Therefore, sober drivers are twice as
likely to die in a traffic accident.&lt;/p&gt;

&lt;p&gt;If you think something is wrong with that argument, you are right, but it&amp;rsquo;s not just because the conclusion
intuitively seems wrong, it&amp;rsquo;s because it involves a mistake in conditional probability. To see the mistake,
it helps to introduce a litle notation, we will define:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;P(D) to be the probability that a person is drunk&lt;/li&gt;
&lt;li&gt;P(A) to be the probability that a person will die in a traffic-related accident&lt;/li&gt;
&lt;li&gt;P(D | A) &lt;em&gt;(pronounced probability of D given A)&lt;/em&gt; is the probability that a person is drunk, given that
there was a death in a traffic-related accident they were in&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So using the 2012 CDC data, we can assign 31%, P(D | A) = 0.31. This is that the probability of a drunk
driver being involved &lt;strong&gt;given that there was a deadly driving accident&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The first thing to point out is that the statement that &amp;lsquo;sober drivers are twice as likely as drunk drivers
to die in an accident&amp;rsquo; is really a statement about P(A | D), that is, the probability of a deadly driving
accident &lt;strong&gt;given that that person is drunk&lt;/strong&gt;. We don&amp;rsquo;t know this yet, however, we can figure it out using
Bayes&amp;rsquo; theorem.&lt;/p&gt;

&lt;h2 id=&#34;bayes-theorem&#34;&gt;Bayes&amp;rsquo; Theorem&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34;&gt;Bayes&amp;rsquo; Theorem&lt;/a&gt; is unusual in that it is extremely useful
and easy to prove, but hard to really understand.
This is something I learned several times in college, but never really understood it&amp;rsquo;s importance until much
later. To see how easy to prove it is, we go back to the definition of conditional probability:&lt;/p&gt;

&lt;p&gt;{% latex %}
$P(X|Y) = P(X \cap Y)/P(Y)$
{% endlatex %}&lt;/p&gt;

&lt;p&gt;Where P(X &amp;cap; Y) is the probability of X and Y occurring. Since this is true for any pair of events X and Y,
we can reverse them and get&lt;/p&gt;

&lt;p&gt;{% latex %}
$P(Y|X) = P(Y \cap X)/P(X)$
{% endlatex %}&lt;/p&gt;

&lt;p&gt;Also, remember that AND is commutative, so that P(X &amp;cap; Y) = P(Y &amp;cap; X), so we can multiply the above two
equations by P(Y) and P(X), respectively, to get:&lt;/p&gt;

&lt;p&gt;{% latex %}
$P(X|Y)P(Y) = P(X \cap Y) = P(Y \cap X) = P(Y|X)P(X)$
{% endlatex %}&lt;/p&gt;

&lt;p&gt;This relates P(X|Y) to P(Y|X), P(X) and P(Y), we can solve the above equation to get:&lt;/p&gt;

&lt;p&gt;{% latex %}
$P(X|Y) = {P(Y|X)P(X)\over P(Y)}$
{% endlatex %}&lt;/p&gt;

&lt;p&gt;And that&amp;rsquo;s it, we took the definition of conditional probability, did a little algebra, and out popped Bayes&amp;rsquo;
theorem, we can now apply this to the above drunk driving fallacy, and calculate the probability that we are
interested in, that is, P(A | D).&lt;/p&gt;

&lt;p&gt;{% latex %}
$P(A|D) = {P(D|A)P(A)\over P(D)}$
{% endlatex %}&lt;/p&gt;

&lt;p&gt;Since we know P(D|A), we just need to find P(A) and P(D). Since the CDC data we are using is annual data,
we need to take the number of casualties from deadly accidents in the United States for the year of 2012 (33,561)
and divide by the number of drivers (211,814,830), that gives an estimate of P(A) = 33,&lt;sup&gt;561&lt;/sup&gt;&amp;frasl;&lt;sub&gt;211&lt;/sub&gt;,814,830 =
0.0001584, which is about 1 in 6,313.&lt;/p&gt;

&lt;p&gt;Next, we need to find the probability that a driver is drunk P(D), we will use the data from the study
referenced in [3], and define &amp;lsquo;drunk&amp;rsquo; to be a BAC of &amp;geq; 0.1%. Then P(D) = 0.00387 or about 1 in 258 (more
on this calculation in the notes below).&lt;/p&gt;

&lt;p&gt;Now that we have:&lt;/p&gt;

&lt;p&gt;P(D|A) = 0.31 (* probability of a driver being drunk, given they were involved in an accident where someone died *),&lt;/p&gt;

&lt;p&gt;P(A) = 0.0001584 (* probability of a driver being involved in an accident where someone died *), and&lt;/p&gt;

&lt;p&gt;P(D) = 0.00387 (* probability of a driver being drunk *)&lt;/p&gt;

&lt;p&gt;We can figure out P(A|D) (* probability of a drunk driver getting into a deadly accident *)&lt;/p&gt;

&lt;p&gt;P(A|D) = P(D|A)P(A)/P(D) = (0.31*0.0001584)/0.00387 = 0.0127 (1.27 %)&lt;/p&gt;

&lt;p&gt;1.27% is not insignificant, it&amp;rsquo;s about half the probability of rolling snake eyes in craps.
Now, let&amp;rsquo;s compare that to sober driving, we just need to calculate P(A|D&lt;sup&gt;c&lt;/sup&gt;). We can use &lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_total_probability&#34;&gt;Kolmogorov&amp;rsquo;s
Theorem of total probability&lt;/a&gt;, shuffle a few terms to
get:&lt;/p&gt;

&lt;p&gt;P(A|D&lt;sup&gt;c&lt;/sup&gt;) = (P(A) - P(A|D)P(D))/P(D&lt;sup&gt;c&lt;/sup&gt;) = (0.0001584 - 0.0127*0.00387)/(1-0.00387) = .000109,
which is about 1 in 9118.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So the probability of getting in a deadly accident, given that you are drunk is 1.27%, and the probability of getting into
a deadly accident, given that you are not drunk is .01%, that means that it is 127 times more likely that you will get into
a deadly accident while drunk.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;[1] Impaired Driving: Get the Facts &lt;em&gt;Centers for Disease Control&lt;/em&gt;
&lt;a href=&#34;http://www.cdc.gov/Motorvehiclesafety/impaired_driving/impaired-drv_factsheet.html&#34;&gt;
[link]
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] Total licensed drivers &lt;em&gt;U.S. Department of Transportation Federal Highway Administration&lt;/em&gt;
&lt;a href=&#34;http://www.fhwa.dot.gov/policyinformation/statistics/2012/dl22.cfma&#34;&gt;
[link]
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Probability of arrest while driving under the influence (George A Beitel, Michael C Sharp, William D Glauz)
&lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1730617/pdf/v006p00158.pdf&#34;&gt;
[link]
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notes on [3], we don&amp;rsquo;t technically have P(D), but we do have P(D|A&lt;sub&gt;1&lt;/sub&gt;), P(A&lt;sub&gt;1&lt;/sub&gt;),
and P(A&lt;sub&gt;1&lt;/sub&gt;|D), where A&lt;sub&gt;1&lt;/sub&gt; is the event that a person is arrested. We can then find
P(D) = (P(D|A&lt;sub&gt;1&lt;/sub&gt;)P(A&lt;sub&gt;1&lt;/sub&gt;))/P(A&lt;sub&gt;1&lt;/sub&gt;|D) = (0.06&amp;times;0.000374)/0.0058 =
.00387&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>USA FREEDOM Act passes house</title>
      <link>/blog/2014/05/23/usa-freedom-act-passes-house/</link>
      <pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/blog/2014/05/23/usa-freedom-act-passes-house/</guid>
      <description>&lt;p&gt;About 9 months ago I wrote about &lt;a href=&#34;/blog/2013/08/13/an-unbalance-of-powers/&#34;&gt;NSA bulk spying and how the way the FISA court is being used upsets the balance of powers in the government&lt;/a&gt;, the &lt;a href=&#34;https://www.govtrack.us/congress/bills/113/hr2586&#34;&gt;two&lt;/a&gt; &lt;a href=&#34;https://www.govtrack.us/congress/bills/113/hr2761&#34;&gt;bills&lt;/a&gt; I referenced have a 4% and 2% chance of being enacted, according to GovTrack. Both of those bills still haven&amp;rsquo;t left the House, and chances are they won&amp;rsquo;t.&lt;/p&gt;

&lt;p&gt;A more promising bill is the USA FREEDOM Act, which just passed the House:&lt;/p&gt;

&lt;script id=&#34;govtrack:widget:bill:113:hr3361:script&#34; src=&#34;https://www.govtrack.us/congress/bills/113/hr3361/widget.js&#34; type=&#34;text/javascript&#34;&gt;&lt;/script&gt;

&lt;p&gt;Unforunately, Rep. Justin Amash noted that the changes to it are so bad, that most of the original co-sponsors are out:&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; lang=&#34;en&#34;&gt;&lt;p&gt;Just how bad were changes to &lt;a href=&#34;https://twitter.com/search?q=%23USAFREEDOMAct&amp;amp;src=hash&#34;&gt;#USAFREEDOMAct&lt;/a&gt;? MAJORITY of 152 cosponsors voted NO. Roll call: &lt;a href=&#34;http://t.co/2kaG5hrmw1&#34;&gt;http://t.co/2kaG5hrmw1&lt;/a&gt; &lt;a href=&#34;http://t.co/13gUpANOEN&#34;&gt;http://t.co/13gUpANOEN&lt;/a&gt;&lt;/p&gt;&amp;mdash; Justin Amash (@repjustinamash) &lt;a href=&#34;https://twitter.com/repjustinamash/statuses/469883039125106689&#34;&gt;May 23, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Also, the &lt;a href=&#34;https://www.eff.org/deeplinks/2014/05/eff-dismayed-houses-gutted-usa-freedom-act&#34;&gt;EFF doesn&amp;rsquo;t approve&lt;/a&gt;. Fortunately, Sen. Patrick Leahy said he intends on &lt;a href=&#34;https://www.leahy.senate.gov/press/comment-of-senator-patrick-leahy-d-vt_chairman-senate-judiciary-committee-on-house-passage-of-the-usa-freedom-act-&#34;&gt;fixing a lot of those problems in the Senate version of the bill&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s easy to get cynical and fatalistic about the erosion of civil liberties and the problems in government, but recognizing the problems and paying attention, putting pressure on congress can make a difference. Stay rational and contact your &lt;a href=&#34;http://www.house.gov/representatives/find/&#34;&gt;representative&lt;/a&gt; and &lt;a href=&#34;https://www.senate.gov/general/contact_information/senators_cfm.cfm&#34;&gt;senators&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Remember that the Congress is the most powerful branch of government, it makes and destroys laws, with or without the President&amp;rsquo;s consent (1). Each and every member of congress cares about what we citizens think, because they wish to be re-elected, and we elect them. They are more accesible than you might think.&lt;/p&gt;

&lt;p&gt;(1) Even if the President vetos a bill, Congress can still override with a &lt;a href=&#34;https://en.wikipedia.org/wiki/Supermajority#United_States&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; supermajority in both houses&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lies Damned Lies and Statistics</title>
      <link>/blog/2013/04/03/lies-damned-lies-and-statistics/</link>
      <pubDate>Wed, 03 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>/blog/2013/04/03/lies-damned-lies-and-statistics/</guid>
      <description>&lt;p&gt;There is a quote, usually attributed to Mark Twain that goes something
like:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;There are three kinds lies.
Lies, Damned Lies, and Statistics.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;My interpretation of this is that statistics are supposed to be the
worst kind of lie, or that the worst kinds of lies use
statistics.&lt;/p&gt;

&lt;p&gt;The thing that bothers me most about this quote (and the innumerable
minor variations of it that get repeated) is that the word
&amp;lsquo;statistics&amp;rsquo; comes at the end.&lt;/p&gt;

&lt;p&gt;Why does that matter? Notice that the list is presented as a sequence,
an increasing sequence of damned-ness, and the presence of the word
&amp;lsquo;statistics&amp;rsquo; at the end is supposed to imply that it is &amp;lsquo;more damned&amp;rsquo;
than damned lies.&lt;/p&gt;

&lt;p&gt;This interpretation bothers me because the implied damned-ness is
based on the initial correlation, and that correlation is only based
on two data points. The quote depends on a misunderstanding of
statistics. Anyone who has studied a little bit of statistics will
know not to trust an inference based on a correlation in a data set
of only two points!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probability of getting a numerical SHA-1 hash</title>
      <link>/blog/2012/07/21/probability-of-getting-a-numerical-sha-1-hash/</link>
      <pubDate>Sat, 21 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>/blog/2012/07/21/probability-of-getting-a-numerical-sha-1-hash/</guid>
      <description>&lt;p&gt;Git uses the &lt;a href=&#34;http://en.wikipedia.org/wiki/SHA-1&#34;&gt;SHA-1 hash function&lt;/a&gt; to ensure the integrity of the data it stores. One important property of this function is that if the input is changed very slightly, the output changes completely. The output is supposed to be indistinguishable from randomness.&lt;/p&gt;

&lt;p&gt;Just yesterday, I was checking a git repository on GitHub, and I noticed
this:&lt;/p&gt;

&lt;p&gt;{% img /images/blogimg/sha1num.png %}&lt;/p&gt;

&lt;p&gt;The 10 character substring of the full SHA-1 hash of the latest commit
happened to be all numerical. How likely is this to happen?&lt;/p&gt;

&lt;p&gt;Well, since good hash functions are supposed to produce output that is indistinguishable from randomness, we can assume they are randomly drawn from the sample space of all 40-character strings over the hexadecimal alphabet {0,1,2,&amp;hellip;,d,e,f}. The probability that a single character chosen from this alphabet is numerical is just &lt;sup&gt;10&lt;/sup&gt;&amp;frasl;&lt;sub&gt;16&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;Since each of the 40 characters is independent and uniformly distributed, the probability that the entire string is numerical is:&lt;/p&gt;

&lt;p&gt;{% latex %}
  $ \Pi_{i=1}^{40}{\frac{10}{16}} = \left( \frac{10}{16} \right)^{40} = 6.8423 \times 10^{-9} $
{% endlatex %}&lt;/p&gt;

&lt;p&gt;That is 0.0000000068423, that is really small, but also, I was only looking at the first 10 characters, so the probability is going to be bigger, it is (&lt;sup&gt;10&lt;/sup&gt;&amp;frasl;&lt;sub&gt;16&lt;/sub&gt;)&lt;sup&gt;10&lt;/sup&gt; = 0.0090949, or 0.9%. In fact, this is big enough that I should expect to see this every 112 commits.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>